<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;zh-TW&quot;,&quot;en&quot;,&quot;de-DE&quot;,&quot;es-ES&quot;,&quot;fr-FR&quot;,&quot;ko&quot;,&quot;default&quot;]"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>机器学习系统设计笔记2--文本的聚类 | 流水的账</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/rss+xml" href="/rss2.xml"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">机器学习系统设计笔记2--文本的聚类</h1><a id="logo" href="/.">流水的账</a><p class="description">戒骄戒躁，脚踏实地。</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/rss2.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">机器学习系统设计笔记2--文本的聚类</h1><div class="post-meta">2015-10-15<span> | </span><span class="category"><a href="/categories/machineLearning/">机器学习</a></span></div><a class="disqus-comment-count" href="/2015/10/15/mldesgin3/#vcomment"><span class="valine-comment-count" data-xid="/2015/10/15/mldesgin3/"></span><span> 条评论</span></a><div class="post-content"><p>最近看完了 Willi Richert的《机器学习系统设计》。书虽然有点薄但也比较全，内容感觉有点偏文本处理，里面介绍了一些文本处理的方法和工具。综合起来看作为机器学习入门还是挺不错的，这里就简单记一下我做的笔记，方便回顾。书中的代码可以通过它说到的<a target="_blank" rel="noopener" href="https://www.packtpub.com/books/content/support/11704">网站</a>下载，这里是第2部分笔记。</p>
<h2 id="第三章-文本的聚类"><a href="#第三章-文本的聚类" class="headerlink" title="第三章 文本的聚类"></a>第三章 文本的聚类</h2><p>这一章主要开始介绍文本的一些处理方法，并介绍了一下聚类的基本知识。</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>20newsgroup数据集是机器学习中的一个标准数据集，他包含18288个帖子，来自不同的新闻组。如果我们假定每个新闻组是一个簇，那么很容易测试出聚类的方法是否有效。这个数据集可以从 <a target="_blank" rel="noopener" href="http://mlcomp.org/datasets/379">MLComp</a> 下载。scikit可以直接对从MLComp上下载的数据进行处理，非常方便。</p>
<h3 id="词袋"><a href="#词袋" class="headerlink" title="词袋"></a>词袋</h3><p>使用聚类前，需要有一个评价两个文章相似度的指标。有一种比较文本相似度的方法是计算编辑距离，但使用编辑距离不够稳健，没有考虑词语的重新排列，而且计算时间较长。我们使用另一种更简单有效的方法来进行比较–词袋（bag-of-word）。它基于简单的词频统计；对每一个文档中的词语，将它出现次数记录下来并表示成一个向量。我们之后可以把他们当作特征向量在后续步骤中使用。这样聚类的流程就变为：</p>
<blockquote>
<ol>
<li>对每个文档提取重要特征，并针对每个文档存储为一个向量</li>
<li>在这些向量上进行聚类计算</li>
<li>确定每个带聚类文档所在的簇</li>
</ol>
</blockquote>
<p>使用scikit的CountVectorizer可以方便的完成词袋。</p>
<figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line"><span class="attribute">vectorizer</span> <span class="operator">=</span> CountVectorizer(min_df<span class="operator">=</span><span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>min_df是一个参数，表示统计出来的如果只出现过一次的词，全部丢掉。类似的还有max_df表示所有出现次数大于这个值的词全部丢掉。</p>
<h3 id="相似度的计算"><a href="#相似度的计算" class="headerlink" title="相似度的计算"></a>相似度的计算</h3><p>两个词向量之间的相似度计算方法：将两个词向量做差，得到的结果计算欧几里得范数($\sqrt{x_1^2+x_2^2+x_3^2+…+x_n^2}$)。使用scipy.linalg.norm()函数可以计算欧几里得范数（最小距离）。这样就得到了第一个相似度计算方法。</p>
<p>根据相似度的计算，可以知道词向量特征越明显，计算得到的相似度越准确，要求我们做到：</p>
<blockquote>
<ol>
<li>词语频次向量的归一化: 解决有很多词反复出现的问题，得到单位长度为1的向量<br>原始词向量的每一项都除以该词向量的欧几里得范数（$||v||$ 其实可以看成是向量的绝对值）</li>
<li>删除不重要的词语（去停用词）<br>像“most”，“a”，“an”这类没有实际意义的词称为停用词，这类词在各个文档中一般都有出现，但并不能突出文档的特征信息，所以把他们去掉来突出文档特征</li>
<li>词干处理<br>对于英语来说，名词、动词这类词在不同的时态下会有不同，如名词的单复数，所有格形式，动词的现在时或过去时等。对于这种问题，可以通过词干处理得到统一的形式，常用的工具有nltk，在nltk中有不同的词干处理器，可以使用SnowballStemmer来进行处理。例如</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">0</span>]: <span class="keyword">import</span> nltk.stem</span><br><span class="line">In [<span class="number">1</span>]: s = nltk.stem.SnowballStemer(<span class="string">&#x27;english&#x27;</span>)</span><br><span class="line">In [<span class="number">2</span>]: s.stem(<span class="string">&#x27;graphics&#x27;</span>)</span><br><span class="line">Out[<span class="number">0</span>]: <span class="string">u&#x27;graphic&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>这样在预处理阶段，需要先进行词干处理，再交给CountVectorizer来处理得到词向量。</p>
</blockquote>
<blockquote>
<ol start="4">
<li>TF-IDF<br>对于很多情况，有一些词语经常大量出现在某些特定环境中，这时他们对于区分不同文档的帮助很小，此时使用基于词频的词袋方法就会出现问题，于是我们采用基于TF-IDF的方法，统计并生成词向量，其中TF是词频，IDF把权重折扣考虑了进去。 他的效果就是词频很高的和词频很低的得分都比较低，处于中间部分的得分比较高。在scikit中，使用TfidfVectorizer(继承自CountVectorizer)可以直接统计基于TF-IDF的词向量。</li>
</ol>
</blockquote>
<p>这样我们的处理工作就做完了。</p>
<p>虽然词袋模型及其扩展简单有效，但仍然有一些缺点：</p>
<blockquote>
<ol>
<li>它并不涵盖词语之间的关联关系。采用之前的向量化方法，文本“Car hits wall”和“Wall hits car”具有相同的特征向量</li>
<li>它没法正确捕捉否定关系。例如，文本“I will eat ice cream”和“I will not eat ice cream”，尽管他们的意思截然相反，但从特征向量来看它们非常相似。这个问题其实很容易解决，只需要既统计单个词语，又考虑bigrams（成对的词语）或者trigrams（每三个词语）即可</li>
<li>对于拼写错误的词语会处理失败。尽管读者能够很清楚地意识到“database”和“databas”传递了相同的意思，但我们的方法却把他们当成完全不同的词语。同义词的表示可能也会带来问题。</li>
</ol>
</blockquote>
<p>虽然它有一些问题，但他的错误率还是可以接受的。</p>
<h3 id="k-means聚类"><a href="#k-means聚类" class="headerlink" title="k-means聚类"></a>k-means聚类</h3><p>k均值聚类在各个机器学习库中都有实现，他的主要流程如下：</p>
<ol>
<li>初始化选出k个任意的样本，并把它们的特征向量当作这些簇的质心。</li>
<li>遍历所有其他的文本，并将按照离他们最近的质心所在的簇分配给他们。</li>
<li>计算各个簇内文本的质心。</li>
<li>如果新计算出来的质心跟原来的质心不同，则用新的质心重复上面2-4步，直到质心不再变化。</li>
</ol>
<p>由于K-means方法需要随机初始化质心，如果初始质心选的好，则很快就能得到全局最优解，但如果质心选的不好，则可能不仅花费的时间较长，而且还很可能陷入局部最优解。这是k-means方法最大的问题。代码可以参考下面的来使用：</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.clusert import KMeans</span><br><span class="line">num_clusters = <span class="number">10</span> # 举个例子，k=<span class="number">10</span>,有<span class="number">10</span>个簇</span><br><span class="line">km = <span class="constructor">KMeans(<span class="params">n_clusters</span> = <span class="params">num_clusters</span>, <span class="params">init</span> = &#x27;<span class="params">random</span>&#x27;, <span class="params">n_init</span> = 1,<span class="params">verbose</span> = 1)</span></span><br><span class="line">km.fit(data) # 训练数据, 假设data是包含了已经提取好特征的n组数据</span><br></pre></td></tr></table></figure>

<p>关于聚类的效果，sklearn中有一个sklearn.metrics的包，包含了各种不同的指标，用来衡量聚类的质量。</p>
<h3 id="k-means算法改进"><a href="#k-means算法改进" class="headerlink" title="k-means算法改进"></a>k-means算法改进</h3><p><strong>1. 算法的计算复杂度分析</strong><br>首先，在样本分配阶段，需要计算kn次误差平方和，计算复杂度为 *O(knd)*。 其次在更新聚类中心阶段，计算复杂度为 *O(nd)*，如果迭代次数为 <em>t</em>， 则算法的计算复杂度为 *O(kndt)*。因此k-means针对样本个数n具有现行的计算复杂度，是一种非常有效的大树据聚类算法。</p>
<p><strong>2. 聚类中心初始化的改进</strong><br>k-means对聚类中心的初始化很敏感，不同初始值会带来不同的聚类结果，一种简单有效的改进方式是David Arthur提出的k-means++算法，概算法能有效的产生初始聚类中心，可以得  <em>O(logk)</em> 的近似解。k-means++的计算复杂度为<em>O(knd)</em> 没有增加过多的计算负担，同时可以更有效的近似到最优解。</p>
<p><strong>3. 类别个数的自适应确定</strong><br>由于k-means事先需要确定聚类个数k，不具备自适应选择能力，选择不合适的k值会严重影响聚类效果。ISODATA算法引入类别的合并和分开机制，通过计算误差平方和最小来实现聚类，能够自适应决定类别个数。在每一次迭代中，ISODATA算法首先在固定类别个数的情况下进行聚类，然后根据设定样本之间的距离阈值进行合并操作，并根据每一组类别中的样本协方差矩阵信息来判断是否分开。</p>
<p><strong>4. 面向非标准正态分布或非均匀样本集的算法改进</strong><br>k-means采取二次欧式距离作为相似性度量，并且假设误差服从标准的正态分布，因此k-means在处理非标准正态分布和非均匀样本集合时聚类效果较差。为了有效客服该局限性假设，k-means被推广到更广义的度量空间。经典的两种改进框架为Kernel k-means和谱聚类Spectral Clustering。</p>
<p>Kernel k-means将样本点$x_i$通过某种映射方式$x_i\rightarrow\phi(x_i)$到新的高维空间$\phi$，在该空间中样本点之间的内积可以通过对应的核函数进行计算，即：<br>$$k(x_i,x_j)&#x3D;\phi(x_i)^T\phi(x_j)$$<br>借助核函数的存在，可以在新空间进行k-means聚类，样本之间的相似性度量就取决于核函数的选择。</p>
<p>谱聚类算法尝试着变换样本的度量空间，首先需要求取样本集合的仿射矩阵，然后计算仿射矩阵的特征向量，利用得到的特征向量进行k-means聚类。仿射矩阵的特征向量隐含地重新定义样本点的相似性。</p>
</div><div id="donate"><link rel="stylesheet" type="text/css" href="/css/donate.css?v=1.0.0"><script type="text/javascript" src="/js/donate.js?v=1.0.0" successtext="复制成功!"></script><a class="pos-f tr3" id="github" href="https://github.com/Kaiyuan/donate-page" target="_blank" title="Github"></a><div id="DonateText">Donate</div><ul class="list pos-f" id="donateBox"><li id="AliPay" qr="/images/zhifubao.jpg"></li><li id="WeChat" qr="/images/weixin.jpg"></li></ul><div class="pos-f left-100" id="QRBox"><div id="MainBox"></div></div></div><div class="post-copyright"><script type="text/javascript" src="/js/copyright.js?v=1.0.0" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copyright.css?v=1.0.0"><p><span>本文标题：</span>机器学习系统设计笔记2--文本的聚类</p><p><span>文章作者：</span>throneclay</p><p><span>发布时间：</span>2015-10-15</p><p><span>最后更新：</span>2022-08-03</p><p><span>原始链接：</span><a href="/2015/10/15/mldesgin3/">http://blog.throneclay.top/2015/10/15/mldesgin3/</a><span class="copy-path"><i class="fa fa-clipboard" data-clipboard-text="http://blog.throneclay.top/2015/10/15/mldesgin3/"></i></span></p><p><span>版权声明：</span>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p></div><br><div class="tags"><a href="/tags/机器学习"><i class="fa fa-tag">机器学习</i></a></div><div class="post-nav"><a class="pre" href="/2015/10/16/mldesgin4/">机器学习系统设计笔记3--主题模型LDA</a><a class="next" href="/2015/10/14/mldesgin2/">机器学习系统设计笔记1--数据分类</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == 'true' ? true : false;
var verify = 'false' == 'true' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'NqMIHIDAGzizQKypAqjrPjre-gzGzoHsz',
  appKey:'cHLz1Wk17aR4Mjr2LvimJRCm',
  placeholder:'评论一下',
  avatar:'wavatar',
  guest_info:guest_info,
  pageSize:'20'
})
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="http://blog.throneclay.top"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img src="/img/avatar.png"/></a><p>To be a better man.</p><a class="info-icon" href="https://github.com/throneclay" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/studyNotes/">学习笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/embeddedSystem/">嵌入式</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/optimization/">性能优化</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/machineLearning/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/deepLearning/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/compiler/">编译器</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/python%E5%A4%9A%E7%BA%BF%E7%A8%8B/" style="font-size: 15px;">python多线程</a> <a href="/tags/%E9%AB%98%E9%A2%91%E4%BA%A4%E6%98%93/" style="font-size: 15px;">高频交易</a> <a href="/tags/MIC/" style="font-size: 15px;">MIC</a> <a href="/tags/MSP430/" style="font-size: 15px;">MSP430</a> <a href="/tags/OpenCL/" style="font-size: 15px;">OpenCL</a> <a href="/tags/NEON/" style="font-size: 15px;">NEON</a> <a href="/tags/openmp/" style="font-size: 15px;">openmp</a> <a href="/tags/SIMD/" style="font-size: 15px;">SIMD</a> <a href="/tags/kernel%E5%BC%80%E5%8F%91/" style="font-size: 15px;">kernel开发</a> <a href="/tags/Deep-learning/" style="font-size: 15px;">Deep learning</a> <a href="/tags/caffe/" style="font-size: 15px;">caffe</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/theano/" style="font-size: 15px;">theano</a> <a href="/tags/raspberryPi/" style="font-size: 15px;">raspberryPi</a> <a href="/tags/llvm/" style="font-size: 15px;">llvm</a> <a href="/tags/pytorch/" style="font-size: 15px;">pytorch</a> <a href="/tags/LSTM/" style="font-size: 15px;">LSTM</a> <a href="/tags/yolo/" style="font-size: 15px;">yolo</a> <a href="/tags/SNAS/" style="font-size: 15px;">SNAS</a> <a href="/tags/archlinux/" style="font-size: 15px;">archlinux</a> <a href="/tags/shadowsocks/" style="font-size: 15px;">shadowsocks</a> <a href="/tags/CUDA/" style="font-size: 15px;">CUDA</a> <a href="/tags/nas/" style="font-size: 15px;">nas</a> <a href="/tags/ipython/" style="font-size: 15px;">ipython</a> <a href="/tags/gitlab/" style="font-size: 15px;">gitlab</a> <a href="/tags/vagrant/" style="font-size: 15px;">vagrant</a> <a href="/tags/mosquitto/" style="font-size: 15px;">mosquitto</a> <a href="/tags/ffmpeg/" style="font-size: 15px;">ffmpeg</a> <a href="/tags/Clion/" style="font-size: 15px;">Clion</a> <a href="/tags/Mac/" style="font-size: 15px;">Mac</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/08/08/confluence_synology/">使用docker部署支持备份，全docker环境的postgres+confluence环境</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/05/21/dsm7_gitlab/">在Synology DSM7.0上，找回我们熟悉的gitlab</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/23/llvm-note/">llvm编译的基本概念和流程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/06/py-multi-threads/">python多线程编程</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/10/libtorch_cpp/">libtorch c++ api上手指南</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/05/yolo-learning2/">yolov3实战指南--训练自有数据集并完成TensorRT推理实践</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/19/yolo-learning/">yolov3论文详解--我们究竟在训练什么</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/17/mqtt_protobuf/">mosquitto+protobuf实战</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/12/gitlab_runner/">你好小助手，帮我买瓶酱油--gitlab-runner搭建小记</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/23/ffmpeg_basic/">ffmpeg几种场景下的用法小记</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/throneclay" title="github" target="_blank">github</a><ul></ul><a href="http://zhangjikai.com/" title="zhangjikai" target="_blank">zhangjikai</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a target="_blank" rel="noopener" href="https://beian.miit.gov.cn/">京ICP备20008730号-1 <br></a><a href="/." rel="nofollow">流水的账.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>